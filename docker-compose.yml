version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: biblelm-backend
    ports:
      - "8000:8000" # Map host port 8000 to container port 8000
    volumes:
      - ./data:/app/data # Mount local data directory into the container
      # Optional: Mount source code for development (reflects changes without rebuilding)
      # - ./backend:/app
    environment:
      # Pass Ollama URL if it's running outside this compose file on the host
      # For Docker Desktop (Windows/Mac):
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      # For Linux Host (if Ollama runs directly on host):
      # - OLLAMA_BASE_URL=http://172.17.0.1:11434 # Find Docker bridge IP if needed
      # Ensure this matches the setting in config.yaml or overrides it via config.py
    # depends_on: # Add dependencies if running Ollama/DB in compose
    #   - ollama # Example
    networks:
      - biblelm_network
    restart: unless-stopped
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"] # Use --reload for development if mounting code


  frontend:
    build:
      context: . # Build using the Dockerfile in the current directory (where this compose file is)
      dockerfile: Dockerfile # Explicitly state the Dockerfile name
    container_name: biblelm-frontend
    ports:
      - "8080:80" # Map host port 8080 to container port 80 (Apache)
    volumes:
      # Mount frontend code for development (optional)
      # Note: Changes to PHP might require restart depending on setup
      - ./index.php:/var/www/html/index.php
      - ./style.css:/var/www/html/style.css
      - ./includes:/var/www/html/includes
      - ./js:/var/www/html/js
      - ./assets:/var/www/html/assets
    depends_on:
      - backend # Ensure backend starts before frontend (useful but not strictly required for API calls)
    environment:
      # Pass the backend URL to the PHP container
      # 'backend' is the service name defined above, Docker Compose handles DNS resolution
      - BACKEND_API_URL=http://backend:8000
    networks:
      - biblelm_network
    restart: unless-stopped
  
  # Optional: Add Ollama service if you want to run it in Docker as well
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama_service
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama # Persist models
  #   # Add GPU support if needed (requires nvidia-container-toolkit on host)
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1 # or 'all'
  #   #           capabilities: [gpu]
  #   restart: unless-stopped

volumes:
  data: # Define the named volume used by the backend service
  # ollama_data: # Define volume for Ollama models

networks:
  default:
    driver: bridge
